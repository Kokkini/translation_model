{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import collections\n",
    "import re\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "40000\n",
      "80000\n",
      "120000\n",
      "160000\n",
      "200000\n",
      "240000\n",
      "280000\n",
      "320000\n",
      "360000\n",
      "loaded word vecs\n"
     ]
    }
   ],
   "source": [
    "vec_file = \"C:/Users/tug64107/Desktop/AI/_data/glove.6B/glove.6B.50d.txt\"\n",
    "fp = open(vec_file,encoding=\"utf-8\")\n",
    "glove_dict = {}\n",
    "glove_words = []\n",
    "glove_vecs = []\n",
    "counter = 0\n",
    "for row in fp.readlines():\n",
    "    if counter%40000==0: print(counter)\n",
    "    counter+=1\n",
    "    row = row.split(\" \")\n",
    "    word = row[0]\n",
    "    vec = np.asarray([float(i) for i in row[1:]])\n",
    "    glove_words.append(word)\n",
    "    glove_vecs.append(vec)\n",
    "    glove_dict[word] = vec\n",
    "glove_vecs = np.asarray(glove_vecs)\n",
    "fp.close()\n",
    "print(\"loaded word vecs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5450, 2926]], dtype=int64)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tree to find nearest word\n",
    "tree = NearestNeighbors(2,algorithm=\"kd_tree\")\n",
    "tree.fit(glove_vecs)\n",
    "tree.kneighbors(glove_dict[\"cat\"].reshape([1,-1]),2,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'twere'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-43-f733f224ae99>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mglove_words\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2926\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mglove_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"twere\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m: 'twere'"
     ]
    }
   ],
   "source": [
    "print(glove_words[2926])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "def load_data(file_name):\n",
    "    fp = open(file_name)\n",
    "    content = []\n",
    "    for row in fp.readlines():\n",
    "        content+=re.findall(\"\\w+|[.,!?;&':]\",row.lower())\n",
    "    content = np.asarray(content)\n",
    "    return content\n",
    "\n",
    "def mark_unknown(word_list, dictionary):\n",
    "    count = 0\n",
    "    for i in range(len(word_list)):\n",
    "        if word_list[i] not in dictionary:\n",
    "            count+=1\n",
    "            word_list[i] = \"unk\"\n",
    "            print(i)\n",
    "    print(\"number of unknown words:\",count)\n",
    "\n",
    "text_file = \"C:/Users/tug64107/Desktop/AI/_data/Shakespeare/Shakespeare.txt\"\n",
    "text = load_data(text_file)\n",
    "mark_unknown(text,glove_dict)\n",
    "print(text[:10])\n",
    "vocab = set(text)\n",
    "print(text.shape)\n",
    "print(\"Number of different words: {}\".format(len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text[260913])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train test split\n",
    "train_size = 200000\n",
    "text_train, text_test = text[:train_size], text[train_size:]\n",
    "test_size = text_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fetch_batch(seq_length, batch_size, text, dictionary):\n",
    "    X = []\n",
    "    y = []\n",
    "    for i in range(batch_size):\n",
    "        index = np.random.randint(text.shape[0]-seq_length-1)\n",
    "        X_words = text[index:index+seq_length]\n",
    "        y_words = text[index+1:index+1+seq_length]\n",
    "        X_vecs = [dictionary[w] for w in X_words]\n",
    "        y_vecs = [dictionary[w] for w in y_words]\n",
    "        X.append(X_vecs)\n",
    "        y.append(y_vecs)\n",
    "    return np.asarray(X), np.asarray(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vec_to_word(vec,tree,glove_words):\n",
    "    indice = tree.kneighbors(vec.reshape([1,-1]),1,False)\n",
    "    return glove_words[indice[0,0]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seq_length: 32\n",
      "learning_rate: 1\n",
      "batch_size: 20\n",
      "n_epochs: 30\n",
      "n_batches: 312\n",
      "Epoch: 0  loss test: 0.5015  loss train: 0.5010\n",
      " k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1\n",
      "time for logging and generating: 11\n",
      "Epoch: 5  loss test: 0.3042  loss train: 0.3013\n",
      " k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one\n",
      "time for logging and generating: 16\n",
      "Epoch: 10  loss test: 0.3644  loss train: 0.3580\n",
      " k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but but\n",
      "time for logging and generating: 18\n",
      "Epoch: 15  loss test: 0.3205  loss train: 0.3225\n",
      " k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "time for logging and generating: 16\n",
      "Epoch: 20  loss test: 0.2993  loss train: 0.4398\n",
      " k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "time for logging and generating: 19\n",
      "Epoch: 25  loss test: 0.2942  loss train: 0.2939\n",
      " k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 k978-1 actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually actually\n",
      "time for logging and generating: 16\n"
     ]
    }
   ],
   "source": [
    "# get log file path\n",
    "now = datetime.utcnow().strftime(\"%y%m%d-%H%M\")\n",
    "run_name = \"run\"\n",
    "root_logdir = \"logs\"\n",
    "logdir_test = \"{}/{}_{}_test\".format(root_logdir,run_name,now)\n",
    "logdir_train = \"{}/{}_{}_train\".format(root_logdir,run_name,now)\n",
    "root_savedir = \"checkpoints\"\n",
    "savedir = \"{}/{}_{}\".format(root_savedir,run_name,now)\n",
    "\n",
    "# hyper parameters\n",
    "vec_size = 50\n",
    "seq_length = 32\n",
    "learning_rate = 1\n",
    "batch_size = 20\n",
    "n_epochs = 30\n",
    "n_batches = int(train_size/batch_size/seq_length)\n",
    "print(\"seq_length:\",seq_length)\n",
    "print(\"learning_rate:\",learning_rate)\n",
    "print(\"batch_size:\",batch_size)\n",
    "print(\"n_epochs:\",n_epochs)\n",
    "print(\"n_batches:\",n_batches)\n",
    "\n",
    "# construction phase\n",
    "tf.reset_default_graph()\n",
    "X = tf.placeholder(tf.float32,[None, seq_length,vec_size])\n",
    "y = tf.placeholder(tf.float32, [None, seq_length, vec_size]) #None*seq_length*vec_size\n",
    "\n",
    "with tf.variable_scope(\"forward\"):\n",
    "    cell_1 = tf.contrib.rnn.LSTMCell(128, use_peepholes=True)\n",
    "    cell_2 = tf.contrib.rnn.LSTMCell(64, use_peepholes=True)\n",
    "    cell_3 = tf.contrib.rnn.LSTMCell(64, use_peepholes=True)\n",
    "\n",
    "    multi_layer_cell = tf.nn.rnn_cell.MultiRNNCell([cell_1,cell_2,cell_3])\n",
    "    h_states, fin_state = tf.nn.dynamic_rnn(multi_layer_cell,X,dtype=tf.float32) #h_states: None*seq_length*32, fin_state: tupple of last c and h states[None*32, None*32]\n",
    "    outputs = tf.layers.dense(h_states,vec_size,name=\"dense\") #None*seq_length*vec_size\n",
    "\n",
    "with tf.variable_scope(\"training\"):\n",
    "    loss = tf.losses.mean_squared_error(y,outputs)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "    \n",
    "with tf.variable_scope(\"logging\"):\n",
    "    writer_test = tf.summary.FileWriter(logdir_test,tf.get_default_graph())\n",
    "    writer_train = tf.summary.FileWriter(logdir_train,tf.get_default_graph())\n",
    "    loss_summary = tf.summary.scalar(\"loss_summary\",loss)\n",
    "    summary = tf.summary.merge_all()\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "# execution phase\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(n_epochs):\n",
    "        if(epoch%5==0):\n",
    "            start = time()\n",
    "            X_test, y_test = fetch_batch(seq_length, int(test_size/seq_length), text_test, glove_dict)\n",
    "            loss_test, summary_test =  sess.run([loss,summary],feed_dict={X:X_test,y:y_test})\n",
    "            writer_test.add_summary(summary_test,epoch)\n",
    "            \n",
    "            X_train, y_train = fetch_batch(seq_length, int(train_size/seq_length), text_train, glove_dict)\n",
    "            loss_train, summary_train = sess.run([loss, summary],feed_dict={X:X_train, y:y_train})\n",
    "            writer_train.add_summary(summary_train,epoch)\n",
    "            \n",
    "            saver.save(sess,savedir+\"/model.ckpt\")\n",
    "            \n",
    "            print(\"Epoch: %d  loss test: %.4f  loss train: %.4f\"%(epoch,loss_test,loss_train))\n",
    "            \n",
    "            # generate sequence\n",
    "            seq = np.zeros([seq_length, vec_size])\n",
    "            story_length = 200\n",
    "            for i_char in range(story_length):\n",
    "                X_batch = np.reshape(seq[-seq_length:], [1, seq_length, vec_size])\n",
    "                gen = sess.run(outputs, feed_dict={X: X_batch})\n",
    "                seq = np.append(seq,gen[0, -1, :].reshape([1, -1]), axis=0)\n",
    "\n",
    "            # print result:\n",
    "            story = \"\"\n",
    "            for i in range(seq.shape[0]):\n",
    "                story += \" \" + vec_to_word(seq[i],tree,glove_words)\n",
    "            print(story)\n",
    "            print(\"time for logging and generating: %d\"%(time()-start))\n",
    "        for batch in range(n_batches):\n",
    "            X_train, y_train = fetch_batch(seq_length, batch_size, text_train, glove_dict)\n",
    "            sess.run(training_op, feed_dict={X:X_train,y:y_train})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
