{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import collections\n",
    "import re\n",
    "import tensorflow as tf\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "40000\n",
      "80000\n",
      "120000\n",
      "160000\n",
      "200000\n",
      "240000\n",
      "280000\n",
      "320000\n",
      "360000\n",
      "loaded word vecs\n"
     ]
    }
   ],
   "source": [
    "vec_file = \"C:/Users/tug64107/Desktop/AI/_data/glove.6B/glove.6B.50d.txt\"\n",
    "fp = open(vec_file,encoding=\"utf-8\")\n",
    "glove_dict = {}\n",
    "glove_words = []\n",
    "glove_vecs = []\n",
    "counter = 0\n",
    "for row in fp.readlines():\n",
    "    if counter%40000==0: print(counter)\n",
    "    counter+=1\n",
    "    row = row.split(\" \")\n",
    "    word = row[0]\n",
    "    vec = np.asarray([float(i) for i in row[1:]])\n",
    "    glove_words.append(word)\n",
    "    glove_vecs.append(vec)\n",
    "    glove_dict[word] = vec\n",
    "glove_vecs = np.asarray(glove_vecs)\n",
    "fp.close()\n",
    "print(\"loaded word vecs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.039369  1.2036    0.35401  -0.55999  -0.52078  -0.66988  -0.75417\n",
      " -0.6534   -0.23246   0.58686  -0.40797   1.2057   -1.11      0.51235\n",
      "  0.1246    0.05306   0.61041  -1.1295   -0.11834   0.26311  -0.72112\n",
      " -0.079739  0.75497  -0.023356 -0.56079  -2.1037   -1.8793   -0.179\n",
      " -0.14498  -0.63742   3.181     0.93412  -0.6183    0.58116   0.58956\n",
      " -0.19806   0.42181  -0.85674   0.33207   0.020538 -0.60141   0.50403\n",
      " -0.083316  0.20239   0.443    -0.060769 -0.42807  -0.084135  0.49164\n",
      "  0.085654]\n"
     ]
    }
   ],
   "source": [
    "print(glove_dict[\"'\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['first' 'citizen' ':' 'before' 'we' 'proceed' 'any' 'further' ',' 'hear']\n",
      "(261029,)\n",
      "Number of different words: 11464\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "def load_data(file_name):\n",
    "    fp = open(file_name)\n",
    "    content = []\n",
    "    for row in fp.readlines():\n",
    "        content+=re.findall(\"\\w+|[.,!?;&':]\",row.lower())\n",
    "    content = np.asarray(content)\n",
    "    return content\n",
    "\n",
    "\n",
    "text_file = \"C:/Users/tug64107/Desktop/AI/_data/Shakespeare/Shakespeare.txt\"\n",
    "text = load_data(text_file)\n",
    "print(text[:10])\n",
    "vocab = set(text)\n",
    "print(text.shape)\n",
    "print(\"Number of different words: {}\".format(len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train test split\n",
    "train_size = 200000\n",
    "text_train, text_test = text[:train_size], text[train_size:]\n",
    "test_size = text_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fetch_batch(seq_length, batch_size, text, dictionary):\n",
    "    X = []\n",
    "    y = []\n",
    "    for i in range(batch_size):\n",
    "        index = np.random.randint(text.shape[0]-seq_length-1)\n",
    "        X_words = text[index:index+seq_length]\n",
    "        y_words = text[index+1:index+1+seq_length]\n",
    "        X_vecs = [dictionary[w] for w in X_words]\n",
    "        y_vecs = [dictionary[w] for w in y_words]\n",
    "        X.append(X_vecs)\n",
    "        y.append(y_vecs)\n",
    "    return np.asarray(X), np.asarray(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get log file path\n",
    "now = datetime.utcnow.strftime(\"%y%m%d-%H%M\")\n",
    "run_name = \"run\"\n",
    "root_logdir = \"/logs\"\n",
    "logdir_test = \"{}/{}_{}_test\".format(root_logdir,run_name,now)\n",
    "logdir_train\n",
    "root_savedir = \"/checkpoints\"\n",
    "savedir = \"{}/{}_{}\".format(root_savedir,run_name,now)\n",
    "\n",
    "# hyper parameters\n",
    "vec_size = 50\n",
    "seq_length = 64\n",
    "learning_rate = 0.1\n",
    "batch_size = 20\n",
    "n_epochs = 30\n",
    "n_batches = int(train_size/batch_size/seq_length)\n",
    "print(\"seq_length:\",seq_length)\n",
    "print(\"learning_rate:\",learning_rate)\n",
    "print(\"batch_size:\",batch_size)\n",
    "print(\"n_epochs:\",n_epochs)\n",
    "print(\"n_batches:\",n_batches)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "X = tf.placeholder(tf.float32,[None, seq_length,vec_size])\n",
    "y = tf.placeholder(tf.int32, [None, seq_length, vec_size]) #None*seq_length*n_outputs\n",
    "\n",
    "with tf.variable_scope(\"forward\"):\n",
    "    cell_1 = tf.nn.rnn_cell.LSTMCell(64,use_peepholes=True,name=\"cell_1\")\n",
    "    cell_2 = tf.nn.rnn_cell.LSTMCell(32,use_peepholes=True,name=\"cell_2\")\n",
    "\n",
    "    multi_layer_cell = tf.nn.rnn_cell.MultiRNNCell([cell_1,cell_2])\n",
    "    h_states, fin_state = tf.nn.dynamic_rnn(multi_layer_cell,X,dtype=tf.float32) #h_states: None*seq_length*32, fin_state: tupple of last c and h states[None*32, None*32]\n",
    "    outputs = tf.layers.dense(h_states,vec_size,name=\"dense\") #None*seq_length*vec_size\n",
    "\n",
    "with tf.variable_scope(\"training\"):\n",
    "    loss = tf.losses.mean_squared_error(Y,outputs)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "    \n",
    "with tf.variable_scope(\"logging\"):\n",
    "    writer_test = tf.summary.FileWriter(logdir,tf.get_default_graph())\n",
    "    writer_train = tf.summary.FileWriter(logdir,tf.get_default_graph())\n",
    "    loss_summary = tf.summary.scalar(\"loss_summary\",loss)\n",
    "    summary = tf.summary.merge_all()\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "#train\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(n_epochs):\n",
    "        for batch in range(n_batches):\n",
    "            X_train, y_train = fetch_batch(seq_length, batch_size, text_train, glove_dict)\n",
    "            sess.run(loss, feed_dict={X:X_train,y:y_train})\n",
    "        if(epoch%5==0):\n",
    "            X_test, y_test = fetch_batch(seq_length, int(test_size/seq_length), text_test, glove_dict)\n",
    "            loss_test, summary_test =  sess.run([loss,summary],feed_dict={X:X_test,y:y_test})\n",
    "            writer_test.add_summary(summary_test,epoch)\n",
    "            \n",
    "            X_train, y_train = fetch_batch(seq_length, int(train_size/seq_length), text_train, glove_dict)\n",
    "            loss_train, summary_train = sess.run([loss, summary],feed_dict={X:X_train, y:y_train})\n",
    "            writer_train.add_summary(summary_train,epoch)\n",
    "            \n",
    "        if(epoch%100==0):\n",
    "            # save\n",
    "            save_path = saver.save(sess, \"./checkpoints_LSTM/model.ckpt\")\n",
    "            print(\"Model saved in path: %s\" % save_path)\n",
    "            # generate sequence\n",
    "            seq_onehot = np.zeros([seq_length, n_inputs])\n",
    "            story_length = 500\n",
    "            for i_char in range(story_length):\n",
    "                X_batch = np.reshape(seq_onehot[-seq_length:], [1, seq_length, n_inputs])\n",
    "                gen_onehot = sess.run(predict_onehot, feed_dict={X: X_batch})\n",
    "                seq_onehot = np.append(seq_onehot, gen_onehot[0, -1, :].reshape([1, -1]), axis=0)\n",
    "\n",
    "            # print result:\n",
    "            story = \"\"\n",
    "            for i in range(seq_onehot.shape[0]):\n",
    "                story += ix_to_char[np.argmax(seq_onehot[i])]\n",
    "            print(story)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
